---
title: "Procedural Learning 110320"
author: "Ola Ozernov-Palchik"
date: "11/4/2020"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Clean and organize, include=FALSE,warning = FALSE}
# Clearn and organize data
#import packages 
Packages <- c("dplyr", "stats", "psych", "ggplot2", "lme4", 
              "gridExtra", "dplyr","caret","tidyverse", "ggplot2",
              "plyr","lmerTest","ggpubr","nlme","emmeans"
)

lapply(Packages, library, character.only = TRUE)
source("corstars_function.R")
####Organizational ####

#set subject directory 
#setwd("~/Dropbox (MIT)/learning_analysis")
setwd("~/Dropbox (MIT)/Github/procedural-learning")

#import pocedural.csv file
d <- read.csv('procedural_data072320.csv',stringsAsFactors = FALSE,skipNul = TRUE,
              blank.lines.skip = TRUE)



groups<-read.csv("abcd_group.csv")
names(groups)[names(groups)=="ID"] <- "PartID"
names(d)[names(d)=="abcd_id"] <- "PartID"
d<-merge(d,groups,'PartID')
SL=read.csv("sl_analysis/all_sl_wide.csv")

#Rename variables and organize

#exclude ineligible participants
d<-d%>%filter(d$PartID!='ABCD_1718')
d<-d%>%filter(d$PartID!='ABCD_1728')

d<-d%>%filter(d$Subgroup=="TYP"| d$Subgroup=="DD")

```

# Sample Size
```{r, echo=FALSE,warning = FALSE}
local({
  .Table <- xtabs(~Subgroup, data=d)
  cat("\nFrequency table:\n")
  print(.Table)
})
```


# Rotary Pursuit

Question for JDE: Factor or continuous for trial?

```{r, include=FALSE}
## Extract and organize tasks
PL<-d[c(1,188,3:4,54:70,71:80)]

rp <- NULL
rp <- data.frame('PartID'= character(),'Subgroup' = character(),'plType' = character(),'trial' = double(),
                      'On' = double(),'Off' = double(),stringsAsFactors=FALSE)

for (i in 1:nrow(PL)) {
  if (PL[i,10] == ""){ #if null go to next line
    next()
  }
  for(k in 6:21){
    on <- as.numeric(trimws(strsplit(PL[i,k],';')[[1]][1]))
    off <- as.numeric(trimws(strsplit(PL[i,k],';')[[1]][2]))
    Subgroup <- PL[i,2]
    if (nrow(rp) == 0) {
      rp[1,] <- c(PL[i,1],as.character(PL[i,2]),'rp',k-5,on,off)    }
    else {
      rp[nrow(rp) + 1,] <- c(PL[i,1],as.character(PL[i,2]),'rp',k-5,on,off)
    }
  }
}

rp$On <- as.numeric(rp$On) 
rp$Off <- as.numeric(rp$Off)
rp$trial <- as.numeric(rp$trial) 

rp2<-rp%>%group_by(PartID,Subgroup,trial)%>%
                         dplyr::mutate(prop_on=On/(On+Off))

plot(rp2$prop_on~rp2$trial)
 # left_join(d)

# Check # of trials per each subject
data_tmp4check <- aggregate(prop_on ~ PartID * Subgroup, FUN=NROW, data = rp2 )
unique(data_tmp4check[data_tmp4check$trial != 16,])
#View(data_tmp4check)
length(unique(d$PartID))
data_tmp4check[data_tmp4check$trial !=16,1]
hist(rp2$prop_on)
```

### Statstical Analysis by Trial
There is no significant group difference in baseline speed for RP.
There is a significant learning effect with increasing time on target on across trials. 
The interaction is significant with faster learning for the Dys group. The results remain the same after controlling for sex, age, and IQ.

```{r, echo=FALSE,warning = FALSE}

t.test(d$rotarypursuit_0_2~d$Subgroup)

rp2$PartID = as.factor(rp2$PartID)
rp2$Subgroup = as.factor(rp2$Subgroup)
m1<-lm(prop_on~Subgroup*trial,
        data=rp2) 
summary(m1)
rp2_age_gender_iq = merge(rp2,d[,c(1,3,4,6)],all.x=TRUE)
m2<-lm(prop_on~Subgroup*trial+background_age+background_sex+kbit_ss,
        data=rp2_age_gender_iq) #after controlling for age, sex, and iq, the results remain the same
summary(m2)
lsmeans(m1, list(pairwise ~ Subgroup|trial), adjust = "tukey") #You can use lsmeans to probe interaction between continous and factor
```

#### Linear mixed-effect modeling:
marginal group differences, significant trial differences. Same results when controlling for age, sex, and IQ
```{r, echo=FALSE,warning = FALSE}
lmerrp1 <- lmer(prop_on~Subgroup*trial + (1|PartID),data=rp2)
lmerrp2 <- lmer(prop_on~Subgroup*trial + (1+trial|PartID),data=rp2)
anova(lmerrp1,lmerrp2) 
summary(lmerrp2)
```

```{r, echo=FALSE,warning = FALSE,include=FALSE}
#controlling for age, sex, and iq
lmerrp1 <- lmer(prop_on~Subgroup*trial +background_age+background_sex+kbit_ss + (1|PartID),data=rp2_age_gender_iq)
lmerrp2 <- lmer(prop_on~Subgroup*trial +background_age+background_sex+kbit_ss + (1+trial|PartID),data=rp2_age_gender_iq)
anova(lmerrp1,lmerrp2)
summary(lmerrp2)
```

## Slope Analysis


```{r, echo=FALSE,warning = FALSE,include=FALSE}
####growth curve analysis
temp = complete.cases(rp2)
rp2 = rp2[temp,]
s <- poly(unique(rp2$trial),2)
rp2[,paste("otrial",1:2,sep="")] <-s[rp2$trial,1:2]
summary(rp2)
m.base <- lmer(prop_on ~(otrial1+otrial2)+(otrial1+otrial2 | PartID),data=rp2,REML=FALSE)
m.0 <- lmer(prop_on ~(otrial1+otrial2)+ Subgroup + (otrial1+otrial2 | PartID),data=rp2,REML=FALSE)
m.1 <- lmer(prop_on ~(otrial1+otrial2)*Subgroup + (otrial1+otrial2 | PartID),data=rp2,REML=FALSE)
anova(m.base,m.0,m.1)
summary(m.1)
rp2$fitted_prop_on = fitted(m.1)  # ZQ note: the growth curve analyses suggested that the two groups are marginally different on the linear term (similar as the analysis above). It is perhaps not necessary to fit a curve. I agree that the linear estimate is pretty good.
```

```{r, echo=FALSE, include=FALSE}
####Extract indiviudal slope
# rp2$trial<-as.numeric(as.character(rp2$trial))
# m1<-lm(prop_on ~ poly(trial, 6, raw = TRUE), data = rp2) #determine how many terms
# summary(m1) #choose
# ggplot(rp2, aes(trial, prop_on) ) +
#   geom_point() +
#   stat_smooth(method = lm, formula = y ~ poly(x, 3, raw = TRUE))
```

```{r, echo=FALSE, include=FALSE}
d_glm_fit_list <- vector(mode = "list", length = nrow(rp)*2)
index <- 0
abcd_ids=unique(rp2$PartID)
b_list=c()
m_list = c()

for (subj in abcd_ids) { #d_export is my list of subject 
  #for (a in c("d", "b")) 
  # index <- index + 1
  #print(subj)
  d_subj <- filter(rp2, PartID==subj)
  subj_model <- lm(formula=On ~ trial, #complettion time(On/Off) ~trial
                    data = d_subj)
  b_list[subj] <- as.numeric(subj_model$coefficients[1]) # coefficient intercept
  m_list[subj] <- as.numeric(subj_model$coefficients[2]) # coefficient slope
  
}

#Off Slopes 
d_glm_fit_list <- vector(mode = "list", length = nrow(rp)*2)
index <- 0

abcd_ids=unique(rp$PartID)
b_list2=c()
m_list2 = c()

for (subj in abcd_ids) { #d_export is my list of subject 
  #for (a in c("d", "b")) 
  # index <- index + 1
  #print(subj)
  d_subj <- filter(rp2, PartID==subj)
  subj_model <- lm(formula=Off ~ poly(trial,3,raw=TRUE), #complettion time(On/Off) ~trial
                    data = d_subj)
  b_list2[subj] <- as.numeric(subj_model$coefficients[1]) # coefficient intercept
  m_list2[subj] <- as.numeric(subj_model$coefficients[2]) # coefficient slope
  
}

#PropOn Slopes

d_glm_fit_list <- vector(mode = "list", length = nrow(rp)*2)
index <- 0

abcd_ids=unique(rp$PartID)
b_list4=c()
m_list4 = c()

for (subj in abcd_ids) { #d_export is my list of subject 
  #for (a in c("d", "b")) 
  # index <- index + 1
  #print(subj)
  d_subj <- filter(rp2, PartID==subj)
  subj_model <- lm(formula=prop_on ~ trial, #complettion time(On/Off) ~trial
                    data = d_subj)
  b_list4[subj] <- as.numeric(subj_model$coefficients[1]) # coefficient intercept
  m_list4[subj] <- as.numeric(subj_model$coefficients[2]) # coefficient slope
}

#Create a new Data Frame containing the slopes
rp_data <- data.frame(PartID=abcd_ids,slopeOn = m_list, slopeOff = m_list2, slopeProp_On = m_list4)
d_rp<-merge(d,rp_data) # the two groups are significantly different in IQ (p = 0.04)
```

```{r, echo=FALSE,include=FALSE}
hist(d_rp$slopeProp_On)
```


## Plot Rotary Pursuit 

```{r, echo=FALSE,warning = FALSE}
rp2$trial<-as.integer(rp2$trial)

rpPlot<-dplyr::summarise(group_by(rp2,Subgroup,trial), n = n(),
                         mean=mean(prop_on,na.rm = T), sd=sd(On/(On+Off),na.rm = T),se = sd/sqrt(n))

ggplot(data = rpPlot, aes(x=trial, y=mean, color = Subgroup))+
  geom_line() +geom_point()  +
  scale_color_manual(values=c('red','blue'))+
  geom_errorbar(aes(ymin=mean-se,ymax=mean+se),
                 width=.1,  size=0.5)+
  scale_x_continuous(breaks=seq(2,16,1))+
  #scale_y_continuous(limits = c(0.2, 0.5))+
  theme(
    axis.title = element_text(family = "Trebuchet MS", size = 20),
    legend.key.size = unit(1, "cm"),
    axis.text.x = element_text(size = 15),
    axis.text.y = element_text(size = 15))  + 
  labs(x = "Trials", y = "Proportion On (secs)") +
  theme(panel.background = element_rect(fill = "white", colour = "grey50")) + 
  theme(axis.line = element_line(arrow = arrow(angle = 15, length = unit(.15,"inches"),type = "closed")))
```

### Alternative plots with fitted curve

```{r, echo=FALSE,warning = FALSE, include=FALSE}
ggplot(rp2, aes(trial, prop_on, color=Subgroup))+
  stat_summary(fun.data=mean_se, geom="pointrange")+
  stat_summary(aes(y=fitted_prop_on),fun.y=mean, geom="line")+
  scale_color_manual(values=c('red','blue'))+
  scale_x_continuous(breaks=seq(2,16,1))+
  theme(
    axis.title = element_text(family = "Trebuchet MS", size = 20),
    legend.key.size = unit(1, "cm"),
    axis.text.x = element_text(size = 15),
    axis.text.y = element_text(size = 15))  + 
  labs(x = "Trials", y = "Proportion On (secs)") +
  theme(panel.background = element_rect(fill = "white", colour = "grey50")) + 
  theme(axis.line = element_line(arrow = arrow(angle = 15, length = unit(.15,"inches"),type = "closed")))
```

## ANCOVA on individual slope
marginal group effect

```{r, echo=FALSE,warning = FALSE}
m2<-lm(slopeProp_On~kbit_ss+Subgroup, data=d_rp)
anova(m2)
lsmeans(m2, list(pairwise ~ Subgroup), adjust = "tukey")
```

## Plotting RP Slope Effects
```{r, echo=FALSE,warning = FALSE}
plot_rp_slope<-d_rp%>%dplyr::select('PartID',"Subgroup","slopeProp_On")
plot_rp_slope<-na.omit(plot_rp_slope)
rp_slope = plot_rp_slope %>%
  dplyr::group_by(PartID, Subgroup) %>%
  dplyr::summarise(mean = mean(slopeProp_On, na.rm = T))

multi.group3 <- 
  rp_slope %>%
  dabestr::dabest(Subgroup,mean, 
         idx = list(c("TYP",'DD')),
         paired = FALSE)
plot(multi.group3, palette=c("blue","red"),rawplot.ylabel = "RP Prop On Slope") 
```

# Mirror Tracing 

```{r, include=FALSE,warning = FALSE}
## Extract and organize
mt <- NULL
mt <- data.frame('PartID'= character(),'Subgroup' = character(),'Type' = character(),'trial' = double(),
                     'time' = double(),'error' = double(),stringsAsFactors=FALSE)
for (i in 1:nrow(PL)) {
  if (PL[i,22] == ""){ #if null go to next line
    next()
  }
  for(k in 22:31){
    print(paste(k,PL[i,k]))
    time <- as.numeric(trimws(strsplit(PL[i,k],';')[[1]][1]))
    error <- as.numeric(trimws(strsplit(PL[i,k],';')[[1]][2]))
    if (nrow(mt) == 0) {
      mt[1,] <- c(PL[i,1],PL[i,2],'mt',k-21,time,error)
    }
    else {
      mt[nrow(mt) + 1,] <- c(PL[i,1],PL[i,2],'mt',k-21,time,error)
    }
  }
}

mt$time <- as.numeric(mt$time) 
mt$error <- as.numeric(mt$error)
mt$trial <- as.numeric(mt$trial)
```


```{r,include=FALSE}

## QC
#Should we exclude outliers?
  
# Check # of trials per each subject
data_tmp4check <- aggregate(error ~ PartID * Subgroup, FUN=NROW, data = mt )
unique(data_tmp4check[data_tmp4check$trial != 10,])
#View(data_tmp4check)
length(unique(d$PartID))
data_tmp4check[data_tmp4check$trial !=10,1]

# Outliers
summary(mt)
mt2<-mt #place holder while deciding on outliers
mt2<-mt2%>%filter(error<95) #3sd above mean
length(unique(mt$PartID))
length(unique(mt2$PartID))
histogram(mt2$error)
mt2<-mt2%>%filter(time<73.51465)
histogram(mt2$time)
mt2$Subgroup<-as.factor(mt2$Subgroup)
```


### Statistical Analysis by Trial 
Significant group differences in learning across trials for time, but not error, with better learning for Typ.  Differences survive after controlling for age, sex, and IQ
```{r, echo=FALSE,warning = FALSE}
m3<-lm(time~Subgroup*trial,
        data=mt2)
summary(m3) #trial is significant

m4<-lm(error~Subgroup*trial,
        data=mt2)

summary(m4) #trial is significant

mt2_age_gender_iq = merge(mt2,d[,c(1,3,4,6)],all.x=TRUE)
m3<-lm(time~Subgroup*trial+background_age+background_sex+kbit_ss,
        data=mt2_age_gender_iq) 
summary(m3)
m4<-lm(error~Subgroup*trial+background_age+background_sex+kbit_ss,
        data=mt2_age_gender_iq) 
summary(m4)
```

### linear mixed modeling
main effect of trial, no effect of subgroups
```{r, echo=FALSE,warning = FALSE,include=FALSE}
lmerm1<-lmer(time~Subgroup*trial+(1+trial|PartID),
        data=mt2)
summary(lmerm1)
```

```{r, echo=FALSE,warning = FALSE,include=FALSE}
lmerm2<-lmer(error~Subgroup*trial+(1+trial|PartID),
        data=mt2)
summary(lmerm2)
```

## MT: Plot Time/Error by Trial

```{r, echo=FALSE,warning = FALSE}
mt2$time <- as.numeric(mt2$time) 
mt2$error <- as.numeric(mt2$error)
mt2$trial <- as.numeric(mt2$trial)

mtTPlot<-dplyr::summarise(group_by(mt2,Subgroup,trial), n = n(),
                         meanT=mean(time,na.rm = T), sd=sd(time,na.rm = T),se = sd/sqrt(n))
mtEPlot<-dplyr::summarise(group_by(mt2,Subgroup,trial), n = n(),
                          meanE=mean(error,na.rm = T), sd=sd(error,na.rm = T),se = sd/sqrt(n))


ggplot(data = mtTPlot, aes(x=trial, y=meanT, color = Subgroup))+
  geom_line() +geom_point()  +
  scale_color_manual(values=c('red','blue'))+
  geom_errorbar(aes(ymin=meanT-se,ymax=meanT+se),
                width=.1,  size=0.5)+
  scale_x_continuous(breaks=seq(0,10,1))+
  #scale_y_continuous(limits = c(0.2, 0.5))+
  theme(
    axis.title = element_text(family = "Trebuchet MS", size = 20),
    legend.key.size = unit(1, "cm"),
    axis.text.x = element_text(size = 15),
    axis.text.y = element_text(size = 15))  + 
  labs(x = "Trials", y = "Completion Time (secs)") +
  theme(panel.background = element_rect(fill = "white", colour = "grey50")) + 
  theme(axis.line = element_line(arrow = arrow(angle = 15, length = unit(.15,"inches"),type = "closed")))
```


```{r, echo=FALSE,warning = FALSE}
ggplot(data = mtEPlot, aes(x=trial, y=meanE, color = Subgroup))+
  geom_line() +geom_point()  +
  scale_color_manual(values=c('red','blue'))+
  geom_errorbar(aes(ymin=meanE-se,ymax=meanE+se),
                width=.1,  size=0.5)+
  scale_x_continuous(breaks=seq(0,10,1))+
  #scale_y_continuous(limits = c(0.2, 0.5))+
  theme(
    axis.title = element_text(family = "Trebuchet MS", size = 20),
    legend.key.size = unit(1, "cm"),
    axis.text.x = element_text(size = 15),
    axis.text.y = element_text(size = 15))  + 
  labs(x = "Trials", y = "Errors") +
  theme(panel.background = element_rect(fill = "white", colour = "grey50")) + 
  theme(axis.line = element_line(arrow = arrow(angle = 15, length = unit(.15,"inches"),type = "closed")))
```

## Mirror Tracing Slopes

Significant group effects for error and time, with steeper slopes in Typ as compared to Dys

```{r, echo=FALSE,warning = FALSE,include=FALSE}

#Error slopes
d_glm_fit_list <- vector(mode = "list", length = nrow(rp)*2)
index <- 0
abcd_ids=unique(mt2$PartID)
b_list5=c()
m_list5 = c()

for (subj in abcd_ids) {
 # print(subj)
  d_subj <- filter(mt, PartID==subj)
  subj_model <- glm(error ~ trial, #errors
                    data = d_subj)
  b_list5[subj] <- as.numeric(subj_model$coefficients[1]) # coefficient intercept
  m_list5[subj] <- as.numeric(subj_model$coefficients[2]) # coefficient slope
}
```


```{r, echo=FALSE,warning = FALSE,include=FALSE}
d_glm_fit_list <- vector(mode = "list", length = nrow(rp)*2)
index <- 0
abcd_ids=unique(mt2$PartID)
b_list7=c()
m_list7 = c()

for (subj in abcd_ids) { #d_export is my list of subject 
  #print(subj)
  d_subj <- dplyr::filter(mt2, PartID==subj)
  subj_model <- lm(formula=time ~ poly(trial,4,raw=TRUE), #completion time
                    data = d_subj)
  b_list7[subj] <- as.numeric(subj_model$coefficients[1]) # coefficient intercept
  m_list7[subj] <- as.numeric(subj_model$coefficients[2]) # coefficient slope
  
}


#new data frame containing the mirror slopes
mt_data <- data.frame(PartID=abcd_ids,slope_me = m_list5, slope_mt = m_list7)
mt_data$slope_me<-as.numeric(mt_data$slope_me)
#mt_data=mt_data%>%dplyr::filter(slope_me>-300)
mt_data$slope_me<-log10(abs(mt_data$slope_me)) #transform slope
hist(mt_data$slope_me)
d2<-merge(d, mt_data)
```

```{r, echo=FALSE,warning = FALSE,include=FALSE} 
d_glm_fit_list <- vector(mode = "list", length = nrow(rp)*2)
index <- 0
abcd_ids=unique(mt2$PartID)
b_list7=c()
m_list7 = c()

for (subj in abcd_ids) { #d_export is my list of subject 
  #print(subj)
  d_subj <- dplyr::filter(mt, PartID==subj)
  subj_model <- glm(time ~ trial, #complettion time(On/Off) ~trial
                    data = d_subj,na.action = 'na.omit')
  b_list7[subj] <- as.numeric(subj_model$coefficients[1]) # coefficient intercept
  m_list7[subj] <- as.numeric(subj_model$coefficients[2]) # coefficient slope
  
}

#new data frame containing the mirror slopes
mt_data <- data.frame(PartID=abcd_ids,slope_me = m_list5, slope_mt = m_list7)
mt_data<-as.data.frame(mt_data)
mt_data$slope_me_t<-abs(as.numeric(mt_data$slope_me))
#mt_data<-mt_data%>%filter(slope_me_t<100)
mt_data$slope_me_t<-log10(mt_data$slope_me_t) #transform slope

mt_data$slope_mt_t<-abs(as.numeric(mt_data$slope_mt))
mt_data$slope_mt_t<-log10(mt_data$slope_mt_t) #transform slope

d_3 <- d %>%
   select("PartID","Subgroup", "background_age","background_sex")

d_mt <- left_join(mt_data, d_3, by = "PartID")
d2<-merge(d_rp,d_mt) 
table(d2$Subgroup)
```

### MT Slope Analysis: 
A significant group effects for slope, with faster learning for Typ, even after controlling for age, sex, and IQ.
If one outlier included, then no significant effect
```{r, echo=FALSE,warning = FALSE}
m3<-lm(slope_mt_t~background_age+background_sex+kbit_ss+Subgroup, data=d2,na.action = na.exclude)
anova(m3)
m4<-lm(slope_me_t~background_age+background_sex+kbit_ss+Subgroup, data=d2,na.action = na.exclude)
anova(m4)
lsmeans(m4, list(pairwise ~ Subgroup), adjust = "tukey")

```

## MT: Plot Slope Effects

```{r, echo=FALSE,warning = FALSE}
mt_slope<-d2%>%dplyr::select("PartID","Subgroup","slope_mt_t")

mt_slope = mt_slope %>%
  dplyr::group_by(PartID, Subgroup) %>%
  dplyr::summarise(mean = mean(slope_mt_t, na.rm = T))

multi.group_mt <- 
 mt_slope %>%
  dabestr::dabest(Subgroup,mean, 
         idx = list(c("TYP","DD")),
         paired = FALSE
  )

plot(multi.group_mt, palette=c("blue","red"),rawplot.ylabel = "MT Time Slope")
```


```{r, echo=FALSE,warning = FALSE}
me_slope<-d2%>%dplyr::select("PartID","Subgroup","slope_me_t")

me_slope = me_slope %>%
  dplyr::group_by(PartID, Subgroup) %>%
  dplyr::summarise(mean = mean(slope_me_t, na.rm = T))

multi.group_me <- 
 me_slope %>%
  dabestr::dabest(Subgroup,mean, 
         idx = list(c("TYP","DD")),
         paired = FALSE
  )

plot(multi.group_me, palette=c("blue","red"),rawplot.ylabel = "MT Time Slope")
```

# Statistical Learning
```{r, echo=FALSE,warning = FALSE}
names(SL)[names(SL) == "subj"] <- "PartID"
d_all<-merge(d,rp_data,all = TRUE)
d_all <-merge(d_all,mt_data,all=TRUE)
d_all <-merge(d_all,SL,all=TRUE)
table(SL$group)
```

## Slope analyses 
(ZQ notes: the group difference analysis should refer to abcd_sl_analysis.pdf)
```{r,echo=FALSE,warning = FALSE}
m5<-lm(aud_slope_scale~Subgroup, data=d_all,na.action = na.exclude)
anova(m5)
```

###ASL Slope Effects 
(ZQ notes: the group difference analysis should refer to abcd_sl_analysis.pdf)
```{r, echo=FALSE,warning = FALSE}
asl_slope<-d_all%>%dplyr::select("PartID","Subgroup","aud_slope_scale")

asl_slope<-na.omit(asl_slope)

asl_slope2 = asl_slope %>%
  dplyr::group_by(PartID, Subgroup) %>%
  dplyr::summarise(mean = mean(aud_slope_scale, na.rm = T))

multi.group_asl <- 
 asl_slope2 %>%
  dabestr::dabest(Subgroup,mean, 
         idx = list(c("TYP","DD")),
         paired = FALSE
  )

plot(multi.group_asl, palette=c("blue","red"),rawplot.ylabel = "ASL Time Slope")


```

##VSL Slope Analysis (ZQ notes: the group difference analysis should refer to abcd_sl_analysis.pdf, both the linear models and the lmer models are available with trials entered as continuous variables)
```{r,echo=FALSE,warning = FALSE}
m6<-lm(vis_slope_scale~background_age+background_sex+Subgroup, data=d_all,na.action = na.exclude)
anova(m6) 
```

###VSL Effect Plot 
(ZQ notes: the group difference analysis should refer to abcd_sl_analysis.pdf, significant group effect)
```{r, echo=FALSE,warning = FALSE}
vsl_slope<-d_all%>%dplyr::select("PartID","Subgroup","vis_slope_scale")

vsl_slope<-na.omit(vsl_slope)

vsl_slope2 = vsl_slope %>%
  dplyr::group_by(PartID, Subgroup) %>%
  dplyr::summarise(mean = mean(vis_slope_scale, na.rm = T))

multi.group_vsl <- 
 vsl_slope2 %>%
  dabestr::dabest(Subgroup,mean, 
         idx = list(c("TYP","DD")),
         paired = FALSE
  )

plot(multi.group_vsl, palette=c("blue","red"),rawplot.ylabel = "VSL Slope")

```

###RT Slope
No significant differences in RT for either task.
```{r,echo=FALSE,warning = FALSE}
m6<-lm(aud_fam_rt~background_age+background_sex+Subgroup, data=d_all,na.action = na.exclude)
anova(m6) 

m7<-lm(vis_fam_rt~background_age+background_sex+Subgroup, data=d_all,na.action = na.exclude)
anova(m7) 
```

# Cross-task correlations
```{r,include=FALSE,warning = FALSE}
d_all_dys = subset(d_all,Subgroup == "DD")
d_all_typ = subset(d_all,Subgroup == "TYP")
task_data<-d_all%>%select(kbit_ss_2,gort_ori_ss_2,ctopp_nonword_raw_2,ctopp_elision_raw_2,ctopp_blending_raw_2,wais_dsb_ss_2,slopeProp_On,slope_mt_t,slope_me_t,vis_slope_scale,aud_slope_scale,,vis_acc,aud_acc,quicksin_snr_loss_2)
task_data_dys<-d_all_dys%>%select(kbit_ss_2,gort_ori_ss_2, wrmt_id_ss_2,wrmt_wa_ss_2,towre_sw_ss_2,towre_pde_ss_2, slopeProp_On, slope_mt,slope_me,aud_acc,vis_acc,aud_slope_scale,vis_slope_scale)
task_data_typ<-d_all_typ%>%select(kbit_ss_2,gort_ori_ss_2, wrmt_id_ss_2,wrmt_wa_ss_2,towre_sw_ss_2,towre_pde_ss_2, slopeProp_On,slope_mt,slope_me,aud_acc,vis_acc,aud_slope_scale,vis_slope_scale)
corstars(task_data,method="pearson")#you need to run the function first
```

###Everyone
```{r, echo=FALSE}
corstars(task_data,method="pearson")#you need to run the function first

```
###Dys only
both the rotary pursuit and ASL accuracy/RT are related to reading

```{r, echo=FALSE}
corstars(task_data_dys,method="pearson")#you need to run the function first
```

###Typ only 
better VSL is related to worse reading...
```{r, echo=FALSE}
corstars(task_data_typ,method="pearson")#you need to run the function first
```
